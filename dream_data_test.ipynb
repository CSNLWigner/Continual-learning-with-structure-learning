{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "#from graphviz import Digraph\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "N = 2**2 # particleok szama\n",
    "sigma_r = .5\n",
    "sig_r_model = .5\n",
    "\n",
    "Tx = 5\n",
    "Ty = 5\n",
    "EM_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def sokatmondo_adat(Tx=Tx,Ty=Ty):\n",
    "  alpha = 90 #x-esek eloszor\n",
    "  sigma_r = .3\n",
    "  gamma = helper.gamma_from_alpha(alpha)\n",
    "  #Tx = 3\n",
    "  z_prior = tfd.MultivariateNormalDiag(loc=[-1,1], scale_diag=[.3,.3]);\n",
    "  z = np.array(z_prior.sample(Tx))\n",
    "  r_noise = tfd.Normal(0, .001).sample(Tx)\n",
    "  r_mean = tf.reduce_sum(tf.multiply(gamma,z),1)\n",
    "  r = r_mean + r_noise\n",
    "  datax = {'z':z,'r':r}\n",
    "\n",
    "  alpha = 0 #utana y-osok\n",
    "  gamma = helper.gamma_from_alpha(alpha)\n",
    "  #Ty = 3\n",
    "  z_prior = tfd.MultivariateNormalDiag(loc=[-1,1], scale_diag=[.3,.3]);\n",
    "  z = np.array(z_prior.sample(Ty))\n",
    "  r_noise = tfd.Normal(0, .001).sample(Ty)\n",
    "  r_mean = tf.reduce_sum(tf.multiply(gamma,z),1)\n",
    "  r = r_mean + r_noise\n",
    "  datay = {'z':z,'r':r}\n",
    "\n",
    "  data = concatenate_data(datax, datay)\n",
    "  z = data['z']\n",
    "  r = np.array(data['r'])\n",
    "\n",
    "  xylabels = ['x']*Tx + ['y']*Ty\n",
    "  return z, r, xylabels\n",
    "\n",
    "def concatenate_data(data1, data2):\n",
    "  z = np.concatenate((data1['z'], data2['z']), 0)\n",
    "  r = np.concatenate((np.array(data1['r']), np.array(data2['r'])))\n",
    "  return {'z': z, 'r': r}\n",
    "\n",
    "def plot_data_xy_labels(data, labels):\n",
    "    plt.scatter(*data['z'].T,c=data['r'])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('z_1')\n",
    "    plt.ylabel('z_2')\n",
    "    plt.axhline(y = 0)\n",
    "    plt.axvline(x = 0)\n",
    "    for label, x, y in zip(labels, data['z'][:, 0], data['z'][:, 1]):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-20, 20),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "z, r, xylabels = sokatmondo_adat(5,5)\n",
    "true_data = {'z':z, 'r':r}\n",
    "#plot_data_xy_labels(true_data, xylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_data_xy_labels(true_data, xylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_x = {'z':true_data['z'][:Tx], 'r':true_data['r'][:Tx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_data_xy_labels(data_x, xylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_from_posterior_samples(N=100, gamma=0, z_prior_type='uniform', sigma_z_prior=1, r_bias=0, sigma_reward=0.1, sigma_bias=0):\n",
    "    if z_prior_type == 'normal':\n",
    "        z_prior = tfd.MultivariateNormalDiag(loc=[0,0], scale_diag=[sigma_z_prior,sigma_z_prior]);\n",
    "    elif z_prior_type == 'uniform':\n",
    "        z_prior = tfd.Uniform([-sigma_z_prior,-sigma_z_prior],[sigma_z_prior,sigma_z_prior])\n",
    "    elif z_prior_type == 'informative':\n",
    "        z_prior = tfd.MultivariateNormalDiag(loc=[-1,1], scale_diag=[.3,.3]);\n",
    "\n",
    "\n",
    "    z = np.array(z_prior.sample(N))\n",
    "\n",
    "    r_noise = tfd.Normal(0, sigma_reward).sample(N)\n",
    "    r_mean = tf.cast(tf.reduce_sum(tf.multiply(gamma,z),1),dtype=tf.float32) + r_bias\n",
    "    r = r_mean + r_noise\n",
    "\n",
    "    return z[0],r[0]\n",
    "  \n",
    "def generate_dream_data_from_posterior_samples(samples,z_prior_type):\n",
    "  z_samples = []\n",
    "  r_samples = []\n",
    "\n",
    "  for gamma_sample in samples:\n",
    "      # generate a single datapoint for each gamma sample\n",
    "      z,r = generate_data_from_posterior_samples(N=1, gamma=gamma_sample, z_prior_type=z_prior_type, sigma_reward=sig_r_model)\n",
    "      z_samples.append(z)\n",
    "      r_samples.append(r)\n",
    "\n",
    "  z_samples = np.array(z_samples)\n",
    "  r_samples = np.array(r_samples)\n",
    "\n",
    "  return {'z': np.array(z_samples), 'r': np.array(r_samples)}\n",
    "\n",
    "def generate_dream_data_set(posterior, T=10, N=2, z_prior_type='uniform'):\n",
    "  '''generates multiple dream data sets'''\n",
    "  dream_data_sets = []\n",
    "  for i in range(N):\n",
    "    samples = posterior.sample(T)\n",
    "    dream_data = generate_dream_data_from_posterior_samples(samples, z_prior_type)\n",
    "    dream_data_sets.append(dream_data)\n",
    "  return dream_data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Compute mllh for 1 task and 2 task models on dream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# mllh on 2D 2 task model\n",
    "# these are from particle_filter.ipynb but is it correct to just mllh?\n",
    "\n",
    "from itertools import chain, combinations\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def model_marginal_llh_analytic(zs, rs, sigma_r, Sigma_0 = np.array([[1., 0.], [0., 1.]]), model = '2d'):\n",
    "    # this is the model marginal likelihood function\n",
    "    # it is validated through 'trial_nonorm_posterior_set_transformed'\n",
    "    # from that function the only step fowrad is to leave the normal in gamma (the gamma posterior) since gamma is marginalized out\n",
    "    if zs.size != 0:\n",
    "      T = np.size(zs,0)\n",
    "      if model == '2d':\n",
    "        assert not np.isscalar(Sigma_0), 'Sigma_0 must be a 2-dimensional array'\n",
    "        detSigma_0 = np.linalg.det(Sigma_0)\n",
    "        Sigma_i_star_invs = []\n",
    "        Sigma_i_invs = []\n",
    "        mu_is = []\n",
    "        y = 1/(2*np.pi)/np.sqrt(np.linalg.det(Sigma_0))\n",
    "        for t in range(T):\n",
    "            z = zs[t]\n",
    "            r = rs[t]\n",
    "            Sigma_i_star_inv = np.array([[z[0]**2/sigma_r**2, z[0]*z[1]/sigma_r**2],[z[0]*z[1]/sigma_r**2, z[1]**2/sigma_r**2]])\n",
    "            Sigma_i_star_invs.append(Sigma_i_star_inv)\n",
    "            if t==0:\n",
    "                Sigma_i_inv = Sigma_i_star_inv + np.linalg.inv(Sigma_0)\n",
    "            else:\n",
    "                Sigma_i_inv = Sigma_i_star_inv + Sigma_i_invs[t-1]\n",
    "            Sigma_i_invs.append(Sigma_i_inv)\n",
    "            Sigma_i = np.linalg.inv(Sigma_i_inv)\n",
    "            if t==0:\n",
    "                mu_i = Sigma_i.dot(z*r/sigma_r**2)\n",
    "            else:\n",
    "                mu_i = Sigma_i.dot(z*r/sigma_r**2 + Sigma_i_invs[t-1].dot(mu_is[t-1]))\n",
    "            mu_is.append(mu_i)\n",
    "            y = y * multivariate_normal.pdf(r, mean = 0, cov = sigma_r**2)\n",
    "        y = y / multivariate_normal.pdf(mu_i, mean = np.array([0,0]), cov = Sigma_i)\n",
    "      else:\n",
    "        '''\n",
    "        Sigma_0 is the standard deviation of the gamma prior\n",
    "        '''\n",
    "        assert np.isscalar(Sigma_0), 'Sigma_0 must be scalar'\n",
    "        if model == 'x':\n",
    "          integral_dim = 1\n",
    "        else:\n",
    "          integral_dim = 0\n",
    "\n",
    "        Sigma_i_star_invs = []\n",
    "        Sigma_i_invs = []\n",
    "        mu_is = []\n",
    "        y = 1/(np.sqrt(2*np.pi))/Sigma_0\n",
    "        for t in range(T):\n",
    "            z = zs[t]\n",
    "            r = rs[t]\n",
    "          \n",
    "            Sigma_i_star_inv = z[integral_dim]**2/sigma_r**2\n",
    "            Sigma_i_star_invs.append(Sigma_i_star_inv)\n",
    "            if t==0:\n",
    "                Sigma_i_inv = Sigma_i_star_inv + 1/Sigma_0**2\n",
    "            else:\n",
    "                Sigma_i_inv = Sigma_i_star_inv + Sigma_i_invs[t-1]\n",
    "            Sigma_i_invs.append(Sigma_i_inv)\n",
    "            Sigma_i = 1/Sigma_i_inv\n",
    "            if t==0:\n",
    "                mu_i = Sigma_i * z[integral_dim]*r/sigma_r**2\n",
    "            else:\n",
    "                mu_i = Sigma_i * (z[integral_dim]*r/sigma_r**2 + Sigma_i_invs[t-1]*mu_is[t-1])\n",
    "            mu_is.append(mu_i)\n",
    "            y = y * multivariate_normal.pdf(r, mean = 0, cov = sigma_r**2)\n",
    "        y = y / multivariate_normal.pdf(mu_i, mean = 0.0, cov = Sigma_i)\n",
    "\n",
    "      return y\n",
    "    else:\n",
    "      return 1.\n",
    "\n",
    "def model_marginal_llh_analytic_2x2D(z, r, sigma_r, Sigma_0_2D = np.array([[1., 0.], [0., 1.]]), verbose = True):\n",
    "  T = z.shape[0]\n",
    "  \n",
    "  indices = np.arange(T)\n",
    "  index_subsets = list(powerset(indices))\n",
    "\n",
    "  mmllh_accumulator = 0.\n",
    "  if verbose:\n",
    "    pbar = tf.keras.utils.Progbar(len(index_subsets))\n",
    "  for index_subset in index_subsets:\n",
    "    z1 = z[list(index_subset)]\n",
    "    r1 = r[list(index_subset)]\n",
    "    \n",
    "    complementer_subset = [item for item in indices if item not in index_subset]\n",
    "    \n",
    "    z2 = z[complementer_subset]\n",
    "    r2 = r[complementer_subset]\n",
    "    \n",
    "    mmllh_accumulator += model_marginal_llh_analytic(z1, r1, sigma_r, Sigma_0 = Sigma_0_2D, model = '2d') \\\n",
    "    * model_marginal_llh_analytic(z2, r2, sigma_r, Sigma_0 = Sigma_0_2D, model = '2d')\n",
    "    \n",
    "    if verbose:\n",
    "      pbar.add(1)\n",
    "      \n",
    "  mmllh_accumulator /= 2**T\n",
    "  return mmllh_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_x = {'z':true_data['z'][:Tx], 'r':true_data['r'][:Tx]}\n",
    "data_EM = {'z':true_data['z'][Tx:Tx+EM_SIZE], 'r':true_data['r'][Tx:Tx+EM_SIZE]}\n",
    "data_fit = {'z':np.concatenate([data_x['z'], data_EM['z']]), 'r':np.concatenate([data_x['r'], data_EM['r']])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Fit 2D 1 task model\n",
    "posterior_params = helper.gamma_posterior_analytic(data_fit['z'], data_fit['r'], sigma_r, Sigma_0=10*np.eye(2))\n",
    "posterior = tfd.MultivariateNormalFullCovariance(loc=posterior_params[0], covariance_matrix=posterior_params[1])\n",
    "\n",
    "# generate samples from gamma posterior\n",
    "samples = posterior.sample(10)\n",
    "\n",
    "# generate N dream data sets\n",
    "dreams = generate_dream_data_set(posterior, T=5, N=10,z_prior_type='informative')\n",
    "\n",
    "#append episodic memories to dreams\n",
    "\n",
    "dreams_plus_EM = []\n",
    "for dream in dreams:\n",
    "    dream_plus_EM = {'z':np.concatenate([dream['z'], data_EM['z']]), 'r':np.concatenate([dream['r'], data_EM['r']])}\n",
    "    dream_plus_EM = dreams_plus_EM.append(dream_plus_EM)\n",
    "dreams = dreams_plus_EM\n",
    "    \n",
    "# mllh on 2D 1 task model\n",
    "onetask_mllhs_dream = [helper.model_marginal_llh_analytic(dream['z'], dream['r'], sigma_r, Sigma_0=np.eye(2)) for dream in dreams]\n",
    "\n",
    "# mllh on 2D 2 task model\n",
    "twotask_mllhs_dream = [model_marginal_llh_analytic_2x2D(dream['z'], dream['r'], sigma_r, Sigma_0_2D = np.array([[1., 0.], [0., 1.]])) for dream in dreams]\n",
    "\n",
    "# mllhs on ground truth data\n",
    "onetask_mllh = helper.model_marginal_llh_analytic(data_fit['z'], data_fit['r'], sigma_r, Sigma_0=np.eye(2))\n",
    "twotask_mllh = model_marginal_llh_analytic_2x2D(data_fit['z'], data_fit['r'], sigma_r, Sigma_0_2D = np.array([[1., 0.], [0., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_data_xy_labels(true_data, xylabels)\n",
    "plt.title(\"true data\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "samples = posterior.sample(300)\n",
    "sns.kdeplot(x=samples[:, 0], y=samples[:, 1])\n",
    "plt.scatter(x=samples[:, 0], y=samples[:, 1])\n",
    "plt.title(\"gamma posterior\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.hist(onetask_mllhs_dream,100)\n",
    "plt.axvline(onetask_mllh, color='gray', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.mean(onetask_mllhs_dream), color='lightblue', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel(\"mllh value, true (black dashed), avg (blue dashed)\")\n",
    "plt.ylabel(\"occurences\")\n",
    "\n",
    "plt.hist(twotask_mllhs_dream,100)\n",
    "plt.axvline(twotask_mllh, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.mean(twotask_mllhs_dream), color='darkblue', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"both tasks, 2 task (dark), 1 task (light)\")\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.hist(onetask_mllhs_dream,100)\n",
    "plt.axvline(onetask_mllh, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.mean(onetask_mllhs_dream), color='b', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel(\"mllh value, true (black dashed), avg (blue dashed)\")\n",
    "plt.ylabel(\"occurences\")\n",
    "plt.title(\"1 x 2D task\")\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.hist(twotask_mllhs_dream,100)\n",
    "plt.axvline(twotask_mllh, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.mean(twotask_mllhs_dream), color='b', linestyle='dashed', linewidth=1)\n",
    "plt.title(\"2 x 2D task\")\n",
    "\n",
    "plt.subplot(3,3,7)\n",
    "plt.plot([\"1x2D\",\"2x2D\"],[onetask_mllh, twotask_mllh])\n",
    "plt.plot([\"1x2D\",\"2x2D\"],[np.mean(onetask_mllhs_dream), np.mean(twotask_mllhs_dream)])\n",
    "plt.legend([\"true\",\"avg dream\"])\n",
    "\n",
    "plt.subplot(3,3,8)\n",
    "plt.bar([\"1x2D\",\"2x2D\"],[onetask_mllh, twotask_mllh])\n",
    "plt.title(\"mllh on true data\")\n",
    "\n",
    "plt.subplot(3,3,9)\n",
    "plt.bar([\"1x2D\",\"2x2D\"],[np.mean(onetask_mllhs_dream), np.mean(twotask_mllhs_dream)])\n",
    "plt.title(\"avg mllh on dream data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#num_subplots = len(dreams)\n",
    "num_subplots = 5\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(num_subplots):\n",
    "    plt.subplot(num_subplots, num_subplots,i+1)\n",
    "    helper.plot_data(dreams[i], labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#num_subplots = len(dreams)\n",
    "num_subplots = 5\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(num_subplots):\n",
    "    plt.subplot(num_subplots, num_subplots,i+1)\n",
    "    plt.scatter(dreams[i]['z'].T[0],dreams[i]['z'].T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "points = data_fit\n",
    "plt.scatter(points['z'].T[0],points['z'].T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "points = data_x\n",
    "plt.scatter(points['z'].T[0],points['z'].T[1])\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n",
    "\n",
    "points = data_EM\n",
    "plt.scatter(points['z'].T[0],points['z'].T[1])\n",
    "\n",
    "points = dreams[0]\n",
    "plt.scatter(points['z'].T[0],points['z'].T[1],alpha = 0.5)\n",
    "\n",
    "plt.legend([\"task 1\", \"EM contents\", \"dreamed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
